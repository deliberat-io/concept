# üëç Qualit√§tssicherung

Die Qualit√§t der Angaben deliberat.io soll sich idealerweise selbst regulieren, also ohne dass von au√üen √ºber Fragen der Wahrheit und Richtigkeit entschieden wird. Es soll auch im Weitesten ohne Hierarchien oder privilegierte Rollen f√ºr Nutzende auskommen. Stattdessen wollen wir mit klaren Regeln eine stabile Selbstregulation erm√∂glichen. Dass der gesamte Inhalt von allen mitentschieden werden soll, √∂ffnet nat√ºrlich Raum f√ºr Missbrauch und Unterwanderung.

## üéØ Qualit√§tsziele

Wom√∂glich die gr√∂√üte Aufgabe wird es sein, eine gute Balance zwischen den Zielen herzustellen, die f√ºr die Qualit√§t der Daten entscheidend sind. Wir definieren bis dato f√ºnf ideelle Ziele: Die Mechanismen der Qualit√§tssicherung sollen ...

1. ... effektiv gegen die gezielte Unterwanderung durch geplante Aktionen und Bots wirken
2. ... sich nicht nachteilig auf tats√§chlich sachliche Diskussionen auswirken
3. ... keinen erheblichen Mehraufwand f√ºr die Nutzung bedeuten (es soll Spa√ü machen)
4. ... nur im Ausnahmefall an inhaltlichen Kriterien festgemacht werden
5. ... transparent, nicht-binded und nachvollziehbar gestaltet sein

Insgesamt soll hier eine realisierbare Struktur f√ºr eine gesunde Diskussionskultur erarbeitet werden.

## ‚öñÔ∏è Bewertungsalgorithmus

Nutzende k√∂nnen auf der Plattform verschiedene Beitr√§ge leisten, indem sie Beziehungen herstellen, eine neue Aussage erstellen oder eine Quelle beschreiben. F√ºr jede Angabe auf der Plattform steht die Frage im Raum: "Wann gelten die Angaben als weitgehend gesichert?" inter der Frage, unter welchen Bedingungen eine Beziehungsangabe als gesichert gilt, steht die Frage, ob die Beitr√§ge, die diese Einordnung treffen, als vertrauensw√ºrdig gelten. Wir wollen einen Formel entwickeln, die verschiedene Ans√§tze zur Qualit√§tssicherung vereint. Die Gewichtung der einzelnen Faktoren wollen wir durch wiederholte Evaluation anhand von beispielhaften Diskussionen kalibrieren, um die oben genannte Balance zu finden (siehe unten). Es folgt eine Auswahl der verschiedenen Ans√§tze, die zurzeit diskutiert werden.

::alert{type="warning"}
‚ö†Ô∏è Wichtig: Hier geht es nicht um die Bewertung oder Abgabe von Meinungen, sondern lediglich um die Richtigkeit von Form und Daten.
::

### üí¨ Angabenspezifische Faktoren

#### Gr√∂√üenverh√§ltnis

Die Menge an User\*innen, die n√∂tig ist, damit eine Angabe als gesichert gilt, sollte sich daran orientieren, in welcher Menge und H√§ufigkeit diese Angabe besucht wird. Bei einem Nischenthema reicht es vielleicht, wenn f√ºnf Leute dieselbe Angabe machen (z.B. "Das Argument XY ist tats√§chlich vom Typ 'Analogie-Argument'"). Bei gro√ü diskutierten Themen wiederum w√§re das Unterwandern dann viel zu einfach. Die Anzahl an sichernden Stimmen sollte sich also verh√§ltnism√§√üig an der Gr√∂√üe der Diskussion orientieren. Wie die Gr√∂√üe effektiv gemessen werden kann (z.B. durch Aufrufe, Klicks, Interaktionen), bleibt zu erforschen.

#### Konkurrenz

Wie hoch das Verh√§ltnis der Beitr√§ge, die gegens√§tzliche Angaben machen, in Abh√§ngigkeit von der Gr√∂√üe sein darf, muss getestet werden. Bei kontroversen Themen k√∂nnten Falscheinstufungen den Sicherungsstatus unterdr√ºcken oder sich gar durchsetzen.

#### Netzwerkabschw√§chung

Hier soll verhindert werden, dass eine vernetzte Gruppe an Menschen gro√üen Einfluss auf Sicherung von Angaben erh√§lt. Stimmen etwa immer wider die gleichen zehn Leute f√ºr oder gegen die Richtigkeit einer Angabe, kann die Gewichtung dieser Stimmen reduziert werden. Das bedeutet vereinfacht, dass die Gewissheit der Richtigkeit steigen w√ºrde, wenn Menschen verschiedener sozialer Gruppen und Bubbles sich einig sind.

#### Vorabpr√ºfungen

Beim Eintragen von neuen Aussagen k√∂nnen verschiedene Methoden angewandt werden, um regelwidriges Verhalten von vornherein auszuschlie√üen. Ein klassisches Beispiel w√§re ein Filter f√ºr diskriminierende Bezeichnungen und Beleidigungen. Das L√∂schen von unliebsamen Meinungen soll auf deliberat.io nicht passieren, verfassungswidrige Inhalte hingegen, sind nat√ºrlich nicht in Ordnung und m√ºssen gefiltert werden. Die Anwendung von KI zur Vorsortierung von neuen Eingaben soll gepr√ºft werden. Diese Methoden sollen Vorarbeit leisten, d√ºrfen aber nicht die alleinige Entscheidungsmacht besitzen.

### üßë Nutzerspezifische Faktoren

Diese Faktoren beziehen sich auf einzelne Nutzer\*innen, die Angaben auf der Plattform machen. Ein m√∂glicher Wert, der sich hieraus berechnet, sollte nicht √∂ffentlich dargestellt werden, da es hier nicht ums Vergleichen mit anderen Menschen geht. Eine Darstellung der einzelnen Kategorien f√ºr die Nutzer\*innen selbst kann in Erw√§gung gezogen werden, um die Gewichtung ihrer Beitr√§ge transparent zu gestalten (etwa: "Beitr√§ge zu Argumenttypen von dir flie√üen mit einem Faktor von 0,3 in die Wertung ein, weil du in den letzten drei Monaten 37 falsche Einordnung get√§tigt hast.").

#### Nutzungsdauer und Beitragsmenge

Dieser Faktor soll die Vertrauensw√ºrdigkeit von Beitr√§gen anhand der Aktivit√§t der Nutzer\*innen beurteilen - nach dem Motto: "Wer lange und/oder viel auf der Plattform mitwirkt, hat ernsthaftes Interesse am Diskurs und sollte wissen, wie es richtig geht." Dieser Faktor sollte nur f√ºr relativ kurze Zeitr√§ume greifen, um keine Privilegien zu schaffen, soll aber auch wirksam verhindern, dass frisch erstellte Accounts (z.B. mehrere von einer Person oder nicht-abgefangene Bot-Accounts) bedeutsamen Einfluss auf das Geschehen nehmen k√∂nnen.

#### Regelwidriges Verhalten

Hier soll das Verhalten auf der Plattform einflie√üen. Hat ein User zum Beispiel h√§ufig Argumente falsch eingeordnet oder f√§lschlicherweise gemeldet, w√ºrden wir ein hohes Potenzial f√ºr eine manipulative Absicht unterstellen. Relevante Fragestellungen f√ºr diesen Faktor sind:

- Wie oft wurden Aktionen des Users als falsch gemeldet und haben tats√§chlich gegen Regeln versto√üen?
- Wie oft hat der User Aktionen als falsch gemeldet, obwohl sie sich als richtig rausstellten?

### üß™ Testen und Kalibrieren

Verschiedene Gewichtungen der Faktoren in der Formel k√∂nnen durch vordefinierte Tests √ºberpr√ºft werden. Ein Zustand der Daten wird daf√ºr als Referenz genommen. Dann werden die Ergebnisse der Formel anhand einer Stichprobe divers ausgew√§hlter Beziehungen √ºberpr√ºft. Wie oft deckt sich die Entscheidung der Formel, ob die Summe der Beitr√§ge eine vertrauensw√ºrdige Angabe machen, mit dem zu erwartenden (richtigen) Wert?

L√ºcken in der Treffsicherheit k√∂nnten durch weitere Tests gefunden werden, indem die Stichprobe der Beziehungen stratifiziert wird. Um die erfolgreichste Berechnung der maschinell zu finden, k√∂nnte ein neuronales Netzwerk mit voreingestellten Knoten und Schichten auf die Tests trainiert werden.

::alert{type="warning"}
‚ö†Ô∏è Das Ganze ist ein Experiment! Ob die Qualit√§tssicherung auf diese Weise gelingen kann, werden wir erst wissen, wenn wir es versucht haben. Wir wollen es aber unbedingt ausprobieren! Wie sch√∂n w√§re es, wenn wir eine Form der sozialen Selbstkontrolle etablieren k√∂nnten, die auf einem vielschichtigen und dennoch transparenten Algorithmus basiert?
::
