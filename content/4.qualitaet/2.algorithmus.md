# ‚öñÔ∏è Bewertungsalgorithmus

Nutzende k√∂nnen auf der Plattform verschiedene Beitr√§ge leisten, indem sie Beziehungen herstellen, eine neue Aussage erstellen oder eine Quelle beschreiben. F√ºr jede Angabe auf der Plattform steht die Frage im Raum: "Wann gelten die Einordnungen als weitgehend gesichert oder vertrauensw√ºrdig?" Wir wollen einen Algorithmus entwickeln, der verschiedene Ans√§tze zur Qualit√§tssicherung vereint. Die Gewichtung der einzelnen Faktoren wollen wir durch wiederholte Evaluation anhand von beispielhaften Diskussionen kalibrieren, um die oben genannte Balance zu finden (siehe unten). Es folgt eine Auswahl der verschiedenen Ans√§tze, die zurzeit diskutiert werden.

::alert{type="warning"}
‚ö†Ô∏è Wichtig: Hier geht es nicht um die Bewertung oder Abgabe von Meinungen, sondern lediglich um die Richtigkeit von Form und Daten.
::

## üßÆ Qualit√§tsfaktoren

### üí¨ angabenspezifisch

#### Gr√∂√üenverh√§ltnis

Die Menge an User\*innen, die n√∂tig ist, damit eine Angabe als gesichert gilt, sollte sich daran orientieren, in welcher Menge und H√§ufigkeit diese Angabe besucht wird. Bei einem Nischenthema reicht es vielleicht, wenn f√ºnf Leute dieselbe Angabe machen (z.B. "Das Argument XY ist tats√§chlich vom Typ 'Analogie-Argument'"). Bei gro√ü diskutierten Themen wiederum w√§re das Unterwandern dann viel zu einfach. Die Anzahl an sichernden Stimmen sollte sich also verh√§ltnism√§√üig an der Gr√∂√üe der Diskussion orientieren. Wie die Gr√∂√üe effektiv gemessen werden kann (z.B. durch Aufrufe, Klicks, Interaktionen), bleibt zu erforschen.

#### Konkurrenz

Wie hoch das Verh√§ltnis der Beitr√§ge, die gegens√§tzliche Angaben machen, in Abh√§ngigkeit von der Gr√∂√üe sein darf, muss getestet werden. Bei kontroversen Themen k√∂nnten Falscheinstufungen den Sicherungsstatus unterdr√ºcken oder sich gar durchsetzen.

#### Netzwerkabschw√§chung

Hier soll verhindert werden, dass eine vernetzte Gruppe an Menschen gro√üen Einfluss auf Sicherung von Angaben erh√§lt. Stimmen etwa immer wider die gleichen zehn Leute f√ºr oder gegen die Richtigkeit einer Angabe, kann die Gewichtung dieser Stimmen reduziert werden. Das bedeutet vereinfacht, dass die Gewissheit der Richtigkeit steigen w√ºrde, wenn Menschen verschiedener sozialer Gruppen und Bubbles sich einig sind.

#### Vorabpr√ºfungen

Beim Eintragen von neuen Aussagen k√∂nnen verschiedene Methoden angewandt werden, um regelwidriges Verhalten von vornherein auszuschlie√üen. Ein klassisches Beispiel w√§re ein Filter f√ºr diskriminierende Bezeichnungen und Beleidigungen. Das L√∂schen von unliebsamen Meinungen soll auf deliberat.io nicht passieren, verfassungswidrige Inhalte hingegen, sind nat√ºrlich nicht in Ordnung und m√ºssen gefiltert werden. Die Anwendung von KI zur Vorsortierung von neuen Eingaben soll gepr√ºft werden. Diese Methoden sollen Vorarbeit leisten, d√ºrfen aber nicht die alleinige Entscheidungsmacht besitzen.

### üßë nutzerspezifisch

Diese Faktoren beziehen sich auf einzelne Nutzer\*innen, die Angaben auf der Plattform machen. Ein m√∂glicher Wert, der sich hieraus berechnet, sollte nicht √∂ffentlich dargestellt werden, da es hier nicht ums Vergleichen mit anderen Menschen geht. Eine Darstellung der einzelnen Kategorien f√ºr die Nutzer\*innen selbst kann in Erw√§gung gezogen werden, um die Gewichtung ihrer Beitr√§ge transparent zu gestalten (etwa: "Beitr√§ge zu Argumenttypen von dir flie√üen mit einem Faktor von 0,3 in die Wertung ein, weil du in den letzten drei Monaten 37 falsche Einordnung get√§tigt hast.").

#### Nutzungsdauer und Beitragsmenge

Dieser Faktor soll die Vertrauensw√ºrdigkeit von Beitr√§gen anhand der Aktivit√§t der Nutzer\*innen beurteilen - nach dem Motto: "Wer lange und/oder viel auf der Plattform mitwirkt, hat ernsthaftes Interesse am Diskurs und sollte wissen, wie es richtig geht." Dieser Faktor sollte nur f√ºr relativ kurze Zeitr√§ume greifen, um keine Privilegien zu schaffen, soll aber auch wirksam verhindern, dass frisch erstellte Accounts (z.B. mehrere von einer Person oder nicht-abgefangene Bot-Accounts) bedeutsamen Einfluss auf das Geschehen nehmen k√∂nnen.

#### Regelwidriges Verhalten

Hier soll das Verhalten auf der Plattform einflie√üen. Hat ein User zum Beispiel h√§ufig Argumente falsch eingeordnet oder f√§lschlicherweise gemeldet, w√ºrden wir ein hohes Potenzial f√ºr eine manipulative Absicht unterstellen. Relevante Fragestellungen f√ºr diesen Faktor sind:

- Wie oft wurden Aktionen des Users als falsch gemeldet und haben tats√§chlich gegen Regeln versto√üen?
- Wie oft hat der User Aktionen als falsch gemeldet, obwohl sie sich als richtig rausstellten?

::alert{type="warning"}
‚ö†Ô∏è Das Ganze ist ein Experiment! Ob die Qualit√§tssicherung auf diese Weise gelingen kann, werden wir erst wissen, wenn wir es versucht haben. Wir wollen es aber unbedingt ausprobieren! Wie sch√∂n w√§re es, wenn wir eine Form der sozialen Selbstkontrolle etablieren k√∂nnten, die auf einem vielschichtigen und dennoch transparenten Algorithmus basiert?
::

<!-- ## Qualit√§t einer Aussage

Qualit√§t vs. Validit√§t vs. Verifizit√§t vs. ...
Aussagenqualit√§t vs. Argumentqualit√§t

Faktoren, die die Qualit√§t einer Aussage abwerten:

- Es gibt gar keine Belege oder daf√ºrsprechende Argumente
- Es gibt weder Belege noch daf√ºrsprechende Argumente mit Mindestqualit√§t

## Bewertung

Bei verwandten Projekten ist die Abstimmung oder Bewertung von Aussagen ein zentrales Element. Bei deliberat.io soll es nicht darum gehen, gegeneinander abzustimmen, sondern um das √úberblicken von verschiedenen Perspektiven. -->
